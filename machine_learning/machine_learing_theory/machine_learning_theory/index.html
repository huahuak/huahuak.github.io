<!DOCTYPE html>
<html lang="en"
  dir="ltr">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>
  
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title> | </title>

    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/var.css">


      <script src="/js/main.js"></script>


<link rel="icon" type="image/ico" href="/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="/favicon.ico">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

</head>

<body>
  <div class="aside-layout">
    <aside id="sidebar"> <header>
  <div style="display: flex;">
    <button id="sidebar-lhs-button" onclick="toggleVisibility()" class="sidebar-ico"></button>
  </div>

  <div>
    <nav aria-label="breadcrumb" class="breadcrumb" style="display: flex; align-items: center;">
  <style>
    ol {
      margin: 0;
    }

    li {
      margin: 0;
    }

    .breadcrumb ol {
      padding-left: 0;
    }

    .breadcrumb li {
      display: inline;
    }

    .breadcrumb li:not(:last-child)::after {
       
      color: var(--blue);
      content: "Â·";
    }
  </style>
  <ol>
    
    <li>
      <a href="/"></a>
    </li>
    
    <li>
      <a href="/machine_learning/">Machine_learnings</a>
    </li>
    
    <li class="active">
      <a aria-current="page" href="/machine_learning/machine_learing_theory/machine_learning_theory/"></a>
    </li>
  </ol>
</nav>
  </div>
</header>

<div class="main-container">
  <nav id="TableOfContents">
  <ul>
    <li><a href="#reference">Reference</a></li>
    <li><a href="#linear-regression">Linear Regression</a>
      <ul>
        <li><a href="#1-prior-knowledge">Â§1 prior knowledge</a></li>
        <li><a href="#2-linear-regression">Â§2 Linear Regression</a></li>
        <li><a href="#3-logistic-regression">Â§3 Logistic Regression</a></li>
        <li><a href="#4-generalized-linear-models">Â§4 Generalized Linear Models</a></li>
      </ul>
    </li>
    <li><a href="#generative-learning-algorithms">Generative learning algorithms</a>
      <ul>
        <li><a href="#what-are-generative-learning-algorithms">What are Generative learning algorithms?</a></li>
        <li><a href="#1-gaussian-discriminant-analysis">Â§1 Gaussian discriminant analysis</a></li>
        <li><a href="#2-naive-bayes">Â§2 Naive bayes</a></li>
      </ul>
    </li>
    <li><a href="#support-vector-machines">Support vector machines</a>
      <ul>
        <li><a href="#margin">Margin</a></li>
        <li><a href="#the-optimal-margin-classifier">The optimal margin classifier</a></li>
        <li><a href="#lagrange-duality">Lagrange duality</a></li>
      </ul>
    </li>
    <li><a href="#neural-network">Neural Network</a>
      <ul>
        <li><a href="#what-is-neural-network">What is Neural Network?</a></li>
        <li><a href="#activation-function">Activation Function</a></li>
        <li><a href="#backpropagation">Backpropagation</a></li>
        <li><a href="#reference-1">Reference</a></li>
      </ul>
    </li>
    <li><a href="#generalization">Generalization</a>
      <ul>
        <li><a href="#bias-variance-of-model">Bias-Variance of Model</a></li>
        <li><a href="#double-descent-phenomenon">Double descent phenomenon</a></li>
        <li><a href="#sample-complexity-bound">Sample complexity bound</a></li>
      </ul>
    </li>
    <li><a href="#regularization">Regularization</a>
      <ul>
        <li><a href="#regularization-1">Regularization</a></li>
        <li><a href="#implicit-regularization-effect">Implicit regularization effect</a></li>
        <li><a href="#bayes-statistics">Bayes statistics</a></li>
      </ul>
    </li>
    <li><a href="#evaluation">Evaluation</a>
      <ul>
        <li></li>
        <li><a href="#cross-validation">Cross Validation</a></li>
        <li><a href="#performance">Performance</a></li>
      </ul>
    </li>
    <li><a href="#k-means-alogrithm">K-means alogrithm</a></li>
    <li><a href="#em-algorithm">EM algorithm</a></li>
  </ul>
</nav>
</div> </aside>
    <div id="right-board">
      <header> <div style="display: flex; align-items: center;">
  <button id="sidebar-rhs-button" style="display: none;" onclick="toggleVisibility()" class="sidebar-ico"></button>
  <a href="/" style="font-size: 2em; color: #222; text-decoration: none;">
    <span> HUAHUA </span>
  </a>
</div>

<div style="display: flex; align-items: center;">
  <button onclick="toggleEdit()" class="edit-ico" style="display: none;"></button>
  <button onclick="toggleSearch()" class="search-ico"></button>
  <a href="https://github.com/huahuak/huahuak.github.io">
    <span class="gh-ico"></span>
  </a>
</div>
 </header>
      <main class="center main-container">
        <link href="/pagefind/pagefind-ui.css" rel="stylesheet">
<script src="/pagefind/pagefind-ui.js"></script>
<style>
  .pagefind-ui__result-thumb.svelte-4xnkmf.svelte-4xnkmf {
    display: none;
  }
</style>
<div id="search" style="display: none; margin-top: 0.5em;"></div>
        

<h1 id="machine-learning-theory">Machine Learning Theory</h1>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://docs.google.com/spreadsheets/d/18pHRegyB0XawIdbZbvkr8-jMfi_2ltHVYPjBEOim-6w/edit#gid=0">CS229 Syllabus</a></p>
<p><a href="cs229_lecture_notes.pdf">cs229_lecture_notes.pdf</a></p>
</blockquote>
<h2 id="linear-regression">Linear Regression</h2>
<h3 id="1-prior-knowledge">Â§1 prior knowledge</h3>
<h4 id="maximum-likelihood-estimation"><strong>M</strong>aximum <strong>L</strong>ikelihood <strong>E</strong>stimation</h4>
<p><em>(<strong>M</strong>aximum <strong>L</strong>ikelihood <strong>E</strong>stimationï¼ŒMLE)</em></p>
<blockquote>
<p><a href="https://zh.wikipedia.org/zh-cn/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0">ä¼¼ç„¶å‡½æ•° Wiki</a></p>
<p><a href="https://en.wikipedia.org/wiki/Likelihood_function">Likelihood function Wiki</a></p>
<p><a href="https://zh.wikipedia.org/zh-cn/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1">æå¤§ä¼¼ç„¶ä¼°è®¡ Wiki</a></p>
</blockquote>
<h4 id="matrix-derivatives">Matrix derivatives</h4>
<h4 id="the-exponential-family">The exponential family</h4>
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Exponential_family">Exponential family Wiki</a></p>
</blockquote>
<h4 id="multinomial-distribution">Multinomial Distribution</h4>
<p>å¤šé¡¹åˆ†å¸ƒçš„æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ä¸ºï¼š</p>
$$
\mathrm{P}\left(\mathrm{X}{1}=\mathrm{k}{1},\mathrm{X}{2}=\mathrm{k}{2},\cdots,\mathrm{X}{\mathrm{n}}=\mathrm{k}{\mathrm{n}}\right)=\dfrac{\mathrm{n}!}{\mathrm{k}{1}!\mathrm{k}{2}!\cdots\mathrm{k}{\mathrm{n}}!}\prod{i=1}^{\mathrm{n}}\mathrm{p}{i}^{\mathrm{k}{i}}\quad,\sum_{i=1}^{\mathrm{n}}\mathrm{k}_{i}=\mathrm{n}
$$
<h3 id="2-linear-regression">Â§2 Linear Regression</h3>
<p><strong>Object function</strong>:</p>
$$
h(x) = \theta^{\mathrm{T}}x;
$$
<p><strong>Cost function:</strong></p>
$$
J(\theta) = \frac{1}{2} \sum\limits_{i=1}^{n} (h_\theta(x_i) - y_i)^2
$$
<p><strong>æ–¹æ³•ä¸€</strong>ï¼Œé€šè¿‡ <em>gradient descent</em> æ–¹æ³•è°ƒæ•´å‚æ•° $\theta$ã€‚</p>
<p>é¦–å…ˆè®¡ç®—æ¢¯åº¦æ–¹å‘ï¼Œ</p>
$$
\begin{aligned}
\frac{\partial}{\partial\theta_j}J(\theta)
&=\frac{\partial}{\partial\theta_j}\frac{1}{2}\left(h_\theta(x)-y\right)^2\\
&=2\cdot\frac{1}{2}\left(h_\theta(x)-y\right)\cdot\frac{\partial}{\partial\theta_j}\left(h_\theta(x)-y\right)\\
&=\left(h_\theta(x)-y\right)\cdot\frac{\partial}{\partial\theta_j}\left(\sum_{i=0}^{d}\theta_ix_i-y\right)\\
&=\left(h_\theta(x)-y\right)x_j
\end{aligned}
$$
<p>æ ¹æ®ä¸‹å¼è¿­ä»£è°ƒæ•´ $\theta$ å‚æ•°ã€‚</p>
$$
\theta_j:=\theta_j+\alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x_j^{(i)}.
$$
<p><strong>æ–¹æ³•äºŒ</strong>ï¼Œé€šè¿‡ <em>closed-form solution</em> ç›´æ¥æ±‚å¾—æœ€ä¼˜å‚æ•°ï¼Œå³<strong>æœ€å°äºŒä¹˜æ³•</strong>ã€‚</p>
<p>é¦–å…ˆè®¡ç®—æŸå¤±å‡½æ•° $J(\theta) = \frac{1}{2} \sum\limits_{i=1}^{n} (h_\theta(x_i) - y_i)^2$  çš„æ¢¯åº¦ï¼Œ</p>
$$
\begin{aligned}
\nabla_\theta J(\theta)
&=\nabla_\theta\frac{1}{2}(X\theta-\vec{y})^T(X\theta-\vec{y})\\
&=\frac{1}{2}\nabla_\theta((X\theta)^T X\theta-(X\theta)^T\vec{y}-\vec{y}^T(X\theta)+\vec{y}^T\vec{y})\\
&=\frac{1}{2}\nabla_\theta(\theta^T(X^T X)\theta-\vec{y}^T(X\theta)-\vec{y}^T(X\theta))\\
&=\frac{1}{2}\nabla_\theta\left(\theta^T(X^T X)\theta-2(X^T\vec{y})^T\theta\right)\\ &=\frac{1}{2}\left(2X^T X\theta-2X^T\vec{y}\right)\\
&=X^T X\theta-X^T\vec{y}\\
\end{aligned}
$$
<p>ä»¤ $\nabla_\theta J(\theta) = 0$ï¼Œå¾—</p>
$$
\theta = (X^TX)^{-1}X^T \overrightarrow{y}
$$
<p>ğŸ’¡ <strong>æ¦‚ç‡è§£é‡Š</strong>ï¼Œä¸ºä»€ä¹ˆé€‰æ‹© $J(\theta) = \frac{1}{2} \sum\limits_{i=1}^{n} (h_\theta(x_i) - y_i)^2$ ä½œä¸º cost function?</p>
<p>æ ¹æ®çº¿æ€§æ¨¡å‹ $h(x) = \theta^{\mathrm{T}}x + \varepsilon$ï¼Œå…¶ä¸­ $\varepsilon$ æ˜¯éšæœºå™ªå£°ï¼Œå‡è®¾ $\varepsilon$ æ»¡è¶³æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œå³ $\varepsilon \sim\mathcal{N}(\mu,\sigma^2), \mu = 0, \sigma = 1$ï¼Œåˆ™æœ‰</p>
$$
p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{{\varepsilon^{(i)}}^2}{2\sigma^2}\right) \\
$$
<p>è‹¥å°†æ ‡ç­¾ $y^{(i)}$ ä¸ $\theta^{\mathrm{T}}x$ è®¤ä¸ºæ˜¯éšæœºå˜é‡ï¼Œ$p(y^{(i)}|x^{(i)};\theta)$ æè¿°äº†å½“ç‰¹å¾ $x$ ç¡®å®šæ—¶å®é™…æ ‡ç­¾ $y^{(i)}$ çš„æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ï¼Œ$\theta^{\mathrm{T}}x$ è¡¨æ˜äº†ç‰¹å¾ $x$ ä¸‹çš„é¢„æµ‹å€¼ã€‚</p>
<p>å› ä¸ºéšæœºå™ªå£° $\varepsilon$ æ»¡è¶³æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œå…¶å¹³å‡å€¼ $\mu = 0$ï¼Œå³è¡¨æ˜å®é™…è¾“å‡º $y^{(i)}$ æ˜¯ä»¥ $\theta^{\mathrm{T}}x$ ä¸ºå¹³å‡å€¼çš„æ­£æ€åˆ†å¸ƒï¼Œåˆ™æœ‰</p>
$$
p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)
$$
<p>ä»ä¼¼ç„¶çš„è§’åº¦è€ƒè™‘ $p(y^{(i)}|x^{(i)};\theta)$ï¼ŒåŸºäºå·²æœ‰çš„æ•°æ®çŸ¥è¯† ($\overrightarrow{y}, X$) è·å¾— $\mathrm{argmax}_{\theta} p(\theta;\overrightarrow{y}, X)$ã€‚æ ¹æ®æå¤§ä¼¼ç„¶æ³•ï¼Œæœ€å¤§åŒ–å‡½æ•°</p>
$$
\begin{aligned}L(\theta)&=\prod_{i=1}^np(y^{(i)}\mid x^{(i)};\theta)\\&=\prod_{i=1}^n\frac1{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right).\end{aligned}
$$
<p>ç­‰ä»·äºæœ€å¤§åŒ–å‡½æ•°</p>
$$
\begin{aligned}
\ell(\theta)& =\log L(\theta)  \\
&=\log\prod_{i=1}^n\frac1{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\\
&=\sum_{i=1}^n\log\frac1{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right) \\
&=n\log\frac1{\sqrt{2\pi}\sigma}-\frac1{\sigma^2}\cdot\frac12\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2.\end{aligned}
$$
<p>ç­‰ä»·äºæœ€å°åŒ–</p>
$$
\frac12\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2.
$$
<p>å³çº¿æ€§å›å½’æ¨¡å‹ä½¿ç”¨çš„ä»£ä»·å‡½æ•°ã€‚</p>
<h3 id="3-logistic-regression">Â§3 Logistic Regression</h3>
<p>æ ¹æ®æå¤§ä¼¼ç„¶æ³•ï¼Œé€»è¾‘å›å½’ä¸­ç”± $ x $ ç»™å‡ºçš„ $ y $ æ»¡è¶³äºŒé¡¹åˆ†å¸ƒ (Bernoulli distribution)ï¼Œæ¦‚ç‡åˆ†å¸ƒä¸º:</p>
$$
\begin{aligned}
p(y|x;\theta)&=(h_\theta(x))^y(1-h_\theta(x))^{1-y} \\ 
h_{\theta}(x)&=g(\theta^T x)
\end{aligned}
$$
<p>è¿™é‡Œ $g(\cdot)$ æ˜¯è”ç³»å‡½æ•°(link function)ï¼Œ$g(\cdot)=\frac{1}{1+e^{-x}}$ï¼Œç§°ä¹‹ä¸º logistic function (æˆ– sigmoid å‡½æ•°)ã€‚</p>
<blockquote>
<p>$g(\cdot)=\frac{1}{1+e^{-x}}$ è¯¥å‡½æ•°çš„å¯¼æ•°æ°ä¸º $g'(x)=g(x)(1-g(x))$</p>
$$\begin{aligned}g^{\prime}(z)& =\frac d{dz}\frac1{1+e^{-z}}\\&=\frac1{(1+e^{-z})^2}(e^{-z}) \\&=-\frac1{(1+e^{-z})}\cdot(1-\frac1{(1+e^{-z})}) \\&=g(z)(1-g(z)).\end{aligned}$$
</blockquote>
<p>ä½¿ $L(\theta)=p(y|x;\theta)$ æœ€å¤§ï¼Œ
</p>
$$
\begin{aligned}
L(\theta)
&=p(\vec{y}\mid X;\theta)\\
&=\prod\limits_{i=1}^np(y^{(i)}\mid x^{(i)};\theta)\\
&=\prod\limits_{i=1}^n\left(h_\theta(x^{(i)})\right)^{y^{(i)}}\left(1-h_\theta(x^{(i)})\right)^{1-y^{(i)}}
\end{aligned}
$$
<p>ä¸ºæ±‚è§£æ–¹ä¾¿ï¼Œå¯¹ $L(\theta)$ å–å¯¹æ•°ï¼Œ</p>
$$
\begin{array}{rcl}
\ell(\theta)
&=&\log L(\theta)\\
&=&\sum_{i=1}^n y^{(i)}\log h(x^{(i)})+(1-y^{(i)})\log(1-h(x^{(i)}))
\end{array}
$$
<p>ä¸è€ƒè™‘ $\sum_{i=1}^n$ æ—¶ï¼Œå¯¹ $\ell(\theta)$ æ±‚å¯¼ï¼Œ</p>
$$
\begin{aligned}
\frac{\partial}{\partial\theta_j}\ell(\theta)
&=\left(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}\right)\frac{\partial}{\partial\theta_j}g(\theta^Tx)\\
&=\left(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}\right)g(\theta^Tx)(1-g(\theta^Tx))\frac{\partial}{\partial\theta_j}\theta^Tx\\
&=\left(y(1-g(\theta^Tx))-(1-y)g(\theta^Tx)\right)x_j\\
&=(y-g(\theta^Tx))\:x_j\\
&=(y-h_\theta(x))x_j
\end{aligned}
$$
<p>æ ¹æ® gradient descent rule æ›´æ–°å‚æ•° $\theta$ã€‚</p>
$$
\begin{aligned}
\theta_j&:=\theta_j+\alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x_j^{(i)}\\
&=\theta_j+\alpha\left(y^{(i)}-\frac{1}{1+\exp^{-(\omega x+b)}}\right)x_j^{(i)}\\
\end{aligned}
$$
<p>è¿™é‡Œå½¢å¼ä¸Šä¸çº¿æ€§æ¨¡å‹ç›¸åŒï¼Œå…¶ä¸­ä¸åŒåœ¨äº $h(\theta)$ï¼Œçº¿æ€§æ¨¡å‹ä¸­ä¸º  $h(\theta)=\omega x+b$ï¼Œæ­¤å¤„ä¸º $h(\theta)=g(\omega x+b)=\frac{1}{1+\exp^{-(\omega x+b)}}$ã€‚</p>
<p><strong>æ‹“å±•å­¦ä¹ </strong>ï¼š</p>
<ol>
<li>è€ƒè™‘ $g(z)=\left\{\begin{array}{ll}1&\mathrm{if~}z\geq0\\0&\mathrm{if~}z<0\end{array}\right.$ æ—¶ï¼Œè¯¥æ¨¡å‹ä¸çº¿æ€§å›å½’å’Œé€»è¾‘å›å½’æ¨¡å‹çš„åŒºåˆ«ã€‚</li>
<li>ç‰›é¡¿æ³•æ˜¯ä¸€ç§é€¼è¿‘æå€¼çš„æ–¹æ³•ï¼Œå‚è€ƒ <a href="https://zh.wikipedia.org/zh-hans/%E7%89%9B%E9%A1%BF%E6%B3%95">ç‰›é¡¿æ³• Wiki</a>ã€‚</li>
</ol>
<h3 id="4-generalized-linear-models">Â§4 Generalized Linear Models</h3>
<p><em>(<strong>G</strong>eneralized <strong>L</strong>inear <strong>M</strong>odels, GLMs)</em></p>
<h4 id="linear-regression-1">Linear Regression</h4>
<h4 id="logistic-regression">Logistic Regression</h4>
<h4 id="softmax-regression">Softmax Regression</h4>
<p>Softmax Regression ç”¨äºå¤šåˆ†ç±»åœºæ™¯ï¼Œè¿™é‡Œæˆ‘ä»¬å‡è®¾å¤šåˆ†ç±»åœºæ™¯ä¸­ $y$ æ»¡è¶³æŒ‡æ•°æ—åˆ†å¸ƒï¼Œå³ $y|x,\theta \sim ExponentialFmaily(\theta$)ï¼Œä¸”è¿™é‡Œçš„æŒ‡æ•°æ—åˆ†å¸ƒä¸ºå¤šé¡¹åˆ†å¸ƒ(Multinomial Distribution)ã€‚</p>
$$
\begin{aligned}
p(y|x;\theta)
&=\phi_1^{1\{y=1\}}\phi_2^{1\{y=2\}}\phi_3^{1\{y=3\}}\cdots\phi_k^{1\{y=k\}}\\
&=\phi_1^{1\{y=1\}}\phi_2^{1\{y=2\}}\phi_3^{1\{y=3\}}\cdots\phi_k^{1-\sum_{i=1}^{k-1}1\{y=i\}}\\
&=\phi_1^{(T(y))_1}\phi_2^{(T(y))_2}\phi_3^{(T(y))3}\cdots\phi_k^{1-\sum{i=1}^{k-1}(T(y))_k}\\
&=\exp((T(y))_1log(\phi_1)+(T(y))_2log(\phi_2)+(T(y))_3log(\phi_3)+\\
&\quad\cdots+(1-\sum_{i=1}^{k-1}(T(y))_k)log(\phi_k))\\
&=\exp((T(y))_1log(\frac{\phi_1}{\phi_k})+(T(y))_2log(\frac{\phi_2}{\phi_k})+(T(y))_3log(\frac{\phi_3}{\phi_k})+\\
&\quad\cdots+(T(y))_klog(\phi_k))\\
&=b(y)\exp(\eta^TT\left(y\right)-a\left(\eta\right))\\
\end{aligned}
$$
<p>æ ¹æ®æŒ‡æ•°æ—åˆ†å¸ƒå½¢å¼å¯çŸ¥ï¼Œ</p>
$$
\begin{array}{rcl}
\eta &=&
\left[\begin{array}{}
\log(\frac{\phi_1}{\phi_k})\\ \log(\frac{\phi_2}{\phi_k})\\ \vdots\\ \log(\frac{\phi_k-1}{\phi_k})
\end{array}\right]\\
a(\eta)&=&-log(\phi_k)\\
b(y)&=&1
\end{array}
$$
<p>è¿™é‡Œå‡è®¾ $T(y)=y;h(\theta)=E[y|x]$ï¼Œåˆ™æœ‰ï¼Œ</p>
$$
\begin{array}{rcl}h_{\theta}(x) &=&\text{E}[T(y)|x;\theta]\\ &=&E\left[\begin{array}{c|c} \begin{matrix} 1\{y=2\}\\ 1\{y=2\}\\ \vdots\\ 1\{y=k-1\} \end{matrix}& x;\theta \end{array}\right]\\ &=&\left[ \begin{matrix} \phi_1\\ \phi_2\\ \vdots\\ \phi_{k-1} \end{matrix} \right]\\ &=&\left[ \begin{matrix} \frac{\exp(\theta_1^Tx)}{\sum_{j=1}^{k}\exp(\theta_j^Tx)}\\ \frac{\exp(\theta_2^Tx)}{\sum_{j=1}^{k}\exp(\theta_j^Tx)}\\ \vdots\\ \frac{\exp(\theta_{k-1}^Tx)}{\sum_{j=1}^{k}\exp(\theta_j^Tx)}\\ \end{matrix} \right]\end{array}
$$
<p>æ ¹æ®å¤šé¡¹åˆ†å¸ƒçš„æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ï¼Œå°† $h(\theta)$ ä»£å…¥æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ï¼Œæ ¹æ®æå¤§ä¼¼ç„¶æ±‚å¯¹åº”çš„ $\theta$ã€‚</p>
$$
\begin{array}{rcl}\ell(\theta)
&=&\sum\limits_{i=1}^n\log p(y^{(i)}|x^{(i)};\theta)\\
&=&\sum\limits_{i=1}^n\log\prod\limits_{l=1}^k\left(\dfrac{e^{\theta_l^Tx^{(i)}}}{\sum\limits_{j=1}^ke^{\theta_j^Tx^{(i)}}}\right)^{1\{y^{(i)}=l\}}\end{array}
$$
<p>å¯ä»¥é€šè¿‡ gradient ascent æˆ– Newton&rsquo;s method è¿›è¡Œæ±‚è§£ã€‚</p>
<h2 id="generative-learning-algorithms">Generative learning algorithms</h2>
<h3 id="what-are-generative-learning-algorithms">What are Generative learning algorithms?</h3>
<p>ç”Ÿæˆå¼æ¨¡å‹å’Œåˆ¤åˆ«å¼æ¨¡å‹çš„åŒºåˆ«åœ¨äºï¼Œåˆ¤åˆ«å¼æ¨¡å‹ç›´æ¥å­¦ä¹  $p(y|x)$ æ¡ä»¶æ¦‚ç‡æ¨¡å‹ï¼Œè€Œç”Ÿæˆå¼æ¨¡å‹å…ˆå­¦ä¹  $p(y)$ å…ˆéªŒæ¦‚ç‡ï¼Œç„¶åæ ¹æ®è´å¶æ–¯å®šç†è·å¾— $p(y|x)$ çš„æ¡ä»¶æ¦‚ç‡æ¨¡å‹ã€‚</p>
$$
\begin{aligned}
p(y|x) &= \frac{p(x|y)p(y)}{p(x)}\\
argmax p(y|x) &= argmax \frac{p(x|y)p(y)}{p(x)} \\
&= argmax {p(x|y)p(y)}
\end{aligned}
$$
<h3 id="1-gaussian-discriminant-analysis">Â§1 Gaussian discriminant analysis</h3>
<blockquote>
<p><a href="https://blog.csdn.net/weixin_42555080/article/details/89508850">GDA -CSDN</a></p>
<p><a href="https://www.cnblogs.com/jcchen1987/p/4424436.html">GDA -åšå®¢å›­</a></p>
<p><a href="https://zh.wikipedia.org/zh-hans/%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83">å¤šå…ƒæ­£æ€åˆ†å¸ƒ -Wiki</a></p>
</blockquote>
<p>å‡è®¾</p>
$$
\begin{aligned}
y&\sim\mathrm{Bernoulli}(\phi)\\
x|y=0&\sim\mathcal{N}(\mu_0,\Sigma)\\
x|y=1&\sim\mathcal{N}(\mu_1,\Sigma)\\
\end{aligned}
$$
<h3 id="2-naive-bayes">Â§2 Naive bayes</h3>
<h2 id="support-vector-machines">Support vector machines</h2>
<p>æœ‰æ•°æ®é›† $S = \{x_i, y_i\}ï¼Œ x_i \in R^n, y_i \in \{-1, 1\}$ï¼Œæ”¯æŒå‘é‡æœºçš„è¶…å¹³é¢å°† $S$ æŒ‰ç…§ $y$ æ­£ç¡®çš„åˆ’åˆ†ä¸ºä¸¤ç±»ã€‚è¶…å¹³é¢ä¸º</p>
$$
w^{T}x + b = 0
$$
<p>é¢„æµ‹å‡½æ•°ä¸º</p>
$$
h_{w, b}(x) = sign(w^{T}x + b)
$$
<h3 id="margin">Margin</h3>
<p>æŸä¸€æ ·æœ¬ç‚¹<strong>å‡½æ•°é—´éš” (function margin)</strong> å®šä¹‰ä¸º</p>
$$
\hat{\gamma}^{(i)}=y^{(i)}(w^Tx^{(i)}+b)
$$
<p>åœ¨ $S$ ä¸Šï¼Œåˆ™æœ‰</p>
$$
\hat{\gamma} = \min_{S} \hat{\gamma}^{(i)}
$$
<p>$|w^Tx^{(i)}+b|$ å¯ç›¸å¯¹åœ°è¡¨ç¤ºæ ·æœ¬ç‚¹åˆ°è¶…å¹³é¢çš„è·ç¦»ï¼Œè€Œ $y^{(i)}$ åˆ™èƒ½è¡¨ç¤ºåˆ†ç±»æ˜¯å¦æ­£ç¡®ï¼Œå› æ­¤ $\hat{\gamma}^{(i)}$ èƒ½å¤Ÿè¡¨ç¤ºç‚¹çš„å‡½æ•°é—´éš”ã€‚</p>
<p>ç­‰æ¯”ä¾‹çš„æ”¾å¤§ $w$ å’Œ $b$ ä¸ä¼šæ”¹å˜è¶…å¹³é¢ï¼Œè€Œ $\hat{\gamma}$ å´ä¼šæˆæ¯”ä¾‹çš„æ”¾å¤§ï¼Œ$\hat{\gamma}$ ä¸èƒ½è¡¡é‡å®é™…çš„è¶…å¹³é¢åˆ°æœ€è¿‘æ ·æœ¬ç‚¹çš„é—´éš”ï¼Œå› æ­¤æœ‰åœ¨ $S$ ä¸Šçš„<strong>å‡ ä½•é—´éš” (geometric margin)</strong></p>
$$
\gamma=\min_{S} y^{(i)}\frac{w^Tx^{(i)}+b}{||w||}
$$
<p>å› æ­¤å‡½æ•°é—´éš”å’Œå‡ ä½•é—´éš”ä¹‹é—´å­˜åœ¨å…³ç³»</p>
$$
\gamma = \frac{\hat{\gamma}}{||w||}
$$
<h3 id="the-optimal-margin-classifier">The optimal margin classifier</h3>
<p>å½“<strong>é—´éš” (margin)</strong> è¶Šå¤§æ—¶ï¼Œæˆ‘ä»¬å¯¹ SVM çš„åˆ†ç±»ç»“æœæ›´æœ‰ä¿¡å¿ƒï¼Œæœ‰</p>
$$
\begin{array}{rl}\max_{\gamma,w,b}&\gamma\\
\mathrm{s.t.}&y^{(i)}(w^Tx^{(i)}+b)\geq\gamma,\quad i=1,\ldots,n\\
&||w||=1.
\end{array}
$$
<p>å…¶ä¸­ $\gamma$ å¯ä»¥æ›¿æ¢ä¸º $\frac{\hat{\gamma}}{||w||}$ï¼Œåˆ™æœ‰ç­‰ä»·</p>
$$
\begin{array}{rl}\min_{w,b}&\frac12||w||^2\\\mathrm{s.t.}&y^{(i)}(w^Tx^{(i)}+b)\geq1,&i=1,\ldots,n\end{array}
$$
<h3 id="lagrange-duality">Lagrange duality</h3>
<h2 id="neural-network">Neural Network</h2>
<h3 id="what-is-neural-network">What is Neural Network?</h3>
<p>ç¥ç»ç½‘ç»œæ˜¯ç”±çº¿æ€§å‡½æ•°å’Œæ¿€æ´»å‡½æ•° (activation function) ç»„åˆæ„æˆçš„å¤šå±‚ç½‘çŠ¶å‡½æ•°ç»“æ„ï¼Œå¯è¡¨ç¤ºä¸º</p>
$$
a^{[k]}=\text{ReLU}(W^{[k]}a^{[k-1]}+b^{[k]}),\forall k=1,\ldots,r-1
$$
<p>å…¶ä¸­ ReLu(Recitified linear function) æ˜¯ä¸€ç§æ¿€æ´»å‡½æ•°ï¼Œ$a^{[k]}$ æ˜¯ç¬¬ $k$ å±‚çš„ç¥ç»å…ƒ ( $a^{[0]}$ ä¸ºè¾“å…¥)ï¼Œ$W^{[k]}$ æ˜¯ç¬¬ $k$ å±‚çš„å‚æ•°ã€‚</p>
<h3 id="activation-function">Activation Function</h3>
<p><strong>ReLU</strong> çº¿æ€§æ•´æµå‡½æ•°çš„å®šä¹‰å¦‚ä¸‹</p>
$$
ReLU(x) = \max\{x, 0\}
$$
<h3 id="backpropagation">Backpropagation</h3>
<h3 id="reference-1">Reference</h3>
<p><a href="http://zh.d2l.ai/index.html">ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹ â€” åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹  2.0.0 documentation</a></p>
<h2 id="generalization">Generalization</h2>
<h3 id="bias-variance-of-model">Bias-Variance of Model</h3>
<p>æœ‰ $S = \{x^{(i)}, y^{(i)}\}_i^n, y^{(i)} = h^{\star}(x^{(i)}) + \xi^{(i)}, \xi \sim N(\mu,\sigma^2)$ï¼Œä» $S$ ä¸­å­¦ä¹ åˆ°æ¨¡å‹ $\hat{h}_{S}(x)$ï¼Œåˆ™</p>
$$
\begin{aligned}
\mathrm{MSE}(x)& =\sigma^2+\mathbb{E}[(h^\star(x)-h_S(x))^2]  \\
&=\sigma^2+(h^\star(x)-h_{\mathrm{avg}}(x))^2+\mathbb{E}[(h_{\mathrm{avg}}-h_S(x))^2] \\
&=\underbrace{\sigma^2}_\text{unavoidable}+\underbrace{(h^\star(x)-h_{\mathrm{avg}}(x))^2}_{\triangleq \text{bias}^2\ }+\underbrace{\mathrm{var}(h_S(x))}_\text{ variance}
\end{aligned}
$$
<h3 id="double-descent-phenomenon">Double descent phenomenon</h3>
<blockquote>
<p><a href="https://arxiv.org/pdf/1912.02292">Deep Double Descent: Where Bigger Models and More Data Hurt - from arxiv</a></p>
</blockquote>
<p>ä»<strong>æ¨¡å‹çš„è§’åº¦</strong>å‡ºå‘ï¼ŒåŒé‡ä¸‹é™ç°è±¡æ˜¯æŒ‡æµ‹è¯•è¯¯å·®éšç€æ¨¡å‹å¤æ‚åº¦çš„æå‡å‡ºç°äºŒæ¬¡é™ä½çš„ç°è±¡ã€‚</p>
<p>ä»<strong>æ ·æœ¬çš„è§’åº¦</strong>å‡ºå‘ï¼ŒåŒé‡ä¸‹é™ç°è±¡æ˜¯æŒ‡ï¼Œæµ‹è¯•è¯¯å·®éšç€æ ·æœ¬æ•°é‡å¢åŠ å‡ºç°äºŒæ¬¡é™ä½çš„ç°è±¡ã€‚</p>
<p><img src="machine_learning_theory.assets/Screenshot_2023-09-13_at_22.02.47.png" alt="Screenshot 2023-09-13 at 22.02.47.png"></p>
<p>ç»¼åˆä¸Šè¿°ä¸¤ç§è§’åº¦ï¼Œå½“æ¨¡å‹å‚æ•°çš„æ•°é‡ $d$ ä¸æ ·æœ¬æ•°é‡ $n$ æ¥è¿‘æ—¶ï¼Œå³ $n \approx d$ï¼Œæµ‹è¯•è¯¯å·®ä¼šè¾¾åˆ°æœ€å¤§å€¼ã€‚</p>
<h3 id="sample-complexity-bound">Sample complexity bound</h3>
<h2 id="regularization">Regularization</h2>
<h3 id="regularization-1">Regularization</h3>
<h3 id="implicit-regularization-effect">Implicit regularization effect</h3>
<h3 id="bayes-statistics">Bayes statistics</h3>
<h2 id="evaluation">Evaluation</h2>
<h4 id="model-select">Model Select</h4>
<h3 id="cross-validation">Cross Validation</h3>
<ol>
<li>hold-out cross validation</li>
<li>k-fold cross validation</li>
<li>levave-one-out cross validation</li>
</ol>
<h3 id="performance">Performance</h3>
<p>æœ‰æŸ¥å‡†ç‡ P(precision) ä¸æŸ¥å…¨ç‡  R(recall)ï¼ŒF1 æŒ‡æ ‡æ˜¯ P å’Œ R çš„è°ƒå’Œå¹³å‡ (harmonic mean)ï¼š,</p>
$$
\frac1{F1}=\frac12\cdot\left(\frac1P+\frac1R\right)
$$
<p>æœ‰ çœŸæ­£ä¾‹ç‡TPR(True Positive Rate)ï¼Œ å‡æ­£ä¾‹ç‡ FPR(False Positive Rate)ï¼Œ</p>
$$
\begin{aligned}
TPR = \frac{TP}{TP + FN}\\
FPR = \frac{FP}{FP+TN}\\
\end{aligned}
$$
<p>ç”± FPR ä¸ºæ¨ªåæ ‡ï¼ŒTPR ä¸ºçºµåæ ‡æ„æˆå—æµ‹è€…å·¥ä½œç‰¹å¾ ROC(Reciver Operating Characteristic)ï¼Œæ›²çº¿ä¸‹é¢ç§¯å³ AUC(Area Under Curve)ã€‚</p>
<p>ğŸ’¡ <strong>å¦‚ä½•ç†è§£ AUC çš„å®é™…æ„ä¹‰ï¼Ÿ</strong></p>
<p>ä¸€èˆ¬åœ°ï¼Œåœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œè®¾æ­£ä¾‹çš„æ•°é‡ä¸º $n^+$ï¼Œè€Œåä¾‹çš„æ•°é‡ä¸º $n^-$ã€‚</p>
<p>æ¨¡å‹ $h(x^{(i)})$ = $P(y^{(i)}=1|x^{(i)})$ï¼Œæœ‰åºé›†åˆ $S = \{x|h(x^{(i)}) < h(x^{(i+1)})\}$ã€‚å‡è®¾ $h(x^{(i)})$  ä¸ºæ­£åä¾‹åˆ†ç•Œé˜ˆå€¼ï¼Œå¯¹æ¯ä¸€ä¸ª $i$ åˆ™æœ‰</p>
$$
\begin{array}{rcl}
FPR^{(0)} &=& 0\\
FPR^{(i)} &=& \left\{\begin{array}{clr}
FPR^{(i-1)},&,& y_i=1\\
FPR^{(i-1)}+\frac{1}{n^-} &,& y_i\neq1\\
\end{array}\right.\\
TPR^{(0)} &=& 0\\
TPR^{(i)} &=& \left\{\begin{array}{clr}
TPR^{(i-1)}+\frac{1}{n^+} &,& y_i=1\\
TPR^{(i-1)},&,& y_i\neq1\\
\end{array}\right.\\ 
\end{array}
$$
<p>åˆ™ ROC æ›²çº¿ç”±ç‚¹é›† $O = \{FPR^{(i)},  TPR^{(i)}\}$ ä¾æ¬¡è¿çº¿æ„æˆã€‚ç®€è€Œè¨€ä¹‹ï¼Œæ¨¡å‹ $h(x)$ è¾“å‡ºçš„æœ‰åºé›†åˆ $S$ äº§ç”Ÿ FPR å’Œ TPRï¼ŒROC æ­£æ˜¯ FPR-TRP æ›²çº¿ã€‚</p>
<p>æ ·æœ¬ç‚¹ä¸ºæ­£ä¾‹æ—¶ï¼ŒROC æ›²çº¿æ²¿ y è½´å¢é•¿ï¼Œåä¹‹æ²¿ x è½´å¢é•¿ã€‚å½“æœ‰åºé›†åˆ $S$ æ’åºåçš„å‰ $n^+$ ä¸ªæ ·æœ¬ç‚¹ä¸­æ­£ä¾‹æ¯”ä¾‹è¶Šå¤§æ—¶ï¼ŒROC æ›²çº¿ä¼˜å…ˆæ²¿ y è½´å¢é•¿ï¼Œåæ²¿ x è½´å¢é•¿ï¼Œåˆ™æ­¤æ—¶ AUC è¶Šå¤§ã€‚</p>
<p>å› æ­¤ AUC å®é™…åæ˜ äº†æ¨¡å‹é¢„æµ‹çš„å‰ n ä¸ªæ­£ä¾‹æ˜¯å¦æ­£ç¡®ï¼Œå³é¢„æµ‹æ­£ä¾‹æ’åºæ˜¯å¦ç¬¦åˆå®é™…æ•°æ®æƒ…å†µã€‚</p>
<h2 id="k-means-alogrithm">K-means alogrithm</h2>
<p>$x$ ä¸ºæ ·æœ¬ç‚¹ï¼Œ$\mu$ ä¸ºèšç°‡ä¸­å¿ƒï¼Œé‡å¤ä¸‹è¿°æ­¥éª¤ç›´è‡³æ”¶æ•›ï¼š</p>
$$
\begin{array}{}
\text{For every i, set}&\\
&c^{(i)}:=\arg\min_j||x^{(i)}-\mu_j||^2 \\
\text{For each j, set}&\\
&\mu_j:=\frac{\sum_{i=1}^n1\{c^{(i)}=j\}x^{(i)}}{\sum_{i=1}^n1\{c^{(i)}=j\}}
\end{array}
$$
<h2 id="em-algorithm">EM algorithm</h2>
<p>æœŸæœ›æå¤§ç®—æ³• EM (Expectation-Maximization)</p>
<p>$x$ ä¸ºè¾“å‡ºå€¼ï¼Œ$z$  ä¸ºéšå˜é‡ (latent variable)ï¼Œ$p(z;\phi)$ æè¿°éšå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒï¼Œ$p(x|z;\mu,\Sigma)$ æè¿° $z$ æ¡ä»¶ä¸‹çš„ $x$ æ¦‚ç‡åˆ†å¸ƒ ï¼Œ$p(x,z;\phi,\mu,\Sigma)$ æè¿°è¾“å‡ºå€¼çš„æ¦‚ç‡åˆ†å¸ƒã€‚</p>
<p>E-step:</p>
$$
\begin{array}{}
\text{(E-step) For each }i,j,\mathrm{~set}&\\
&w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)
\end{array}
$$
<p>M-step:</p>
$$
\begin{aligned}&\phi_j\quad:=\quad\frac1n\sum_{i=1}^nw_j^{(i)},\\&\mu_j\quad:=\quad\frac{\sum_{i=1}^nw_j^{(i)}x^{(i)}}{\sum_{i=1}^nw_j^{(i)}},\\&\Sigma_j\quad:=\quad\frac{\sum_{i=1}^nw_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^nw_j^{(i)}}\end{aligned}
$$



        <style>
  .utterances {
    margin-top: 30svh;
    border-top: 1px solid var(--gray);
  }
</style>

<div id="utterances-container"></div>
<script>
  var repo = "huahuak/huahuak.github.io";
  var issueTerm = btoa(location.pathname);
  var theme = "github-light";
  (function () {
    var container = document.getElementById("utterances-container");
    var script = document.createElement("script");
    script.src = "https://utteranc.es/client.js";
    script.setAttribute("repo", repo);
    script.setAttribute("issue-term", issueTerm);
    script.setAttribute("theme", theme);
    script.crossorigin = "anonymous";
    script.async = true;
    container.appendChild(script);
  })();

</script>
      </main>
      <footer>  </footer>
    </div>
  </div>
</body>







<script>
  
  
  
  const localPrefix = "http://localhost:8080/Users/huahua/Library/Mobile%20Documents/com~apple~CloudDocs/HUAHUA/iNOTE";
  var localFunc = []
  window.onload = () => {
    fetch('http://localhost:8080/ping')
      .then((_1, _2) => { localFunc.forEach((f, _1, _2) => f()) })
      .catch(error => console.error('Error:', error));
  }

  
  
  
  function toggleVisibility() {
    let lbtn = document.getElementById("sidebar-lhs-button")
    let rbtn = document.getElementById("sidebar-rhs-button")
    var asideLayout = document.querySelector("div.aside-layout");
    var sidebar = document.getElementById("sidebar");
    if (sidebar.style.display === "none") {
      sidebar.style.display = "block"; 
      var rootStyles = getComputedStyle(document.documentElement);
      var asidePercent = rootStyles.getPropertyValue('--aside-percent');
      asideLayout.style.gridTemplateColumns = asidePercent;

      lbtn.style.display = "block"
      rbtn.style.display = "none"
    } else {
      sidebar.style.display = "none"; 
      asideLayout.style.gridTemplateColumns = '100svw';

      rbtn.style.display = "block"
      lbtn.style.display = "none"
    }
  }

  
  
  
  var oldScroll = 0
  function toggleSearch() {
    let search = document.getElementById("search")
    var main = document.getElementsByTagName("main")[0]
    if (search.style.display !== 'none') {
      search.style.display = 'none'
      main.scrollTop = oldScroll
    } else {
      search.style.display = 'block'
      document.getElementsByTagName('input')[0].focus()
      oldScroll = main.scrollTop
      main.scrollTop = 0
    }
  }

  
  
  
  var path = 'machine_learning\/machine_learing_theory\/machine_learning_theory.md'
  localFunc.push(() => {
    document.getElementsByClassName("edit-ico")[0].style.display = "block"
  })
  function toggleEdit() {
    fetch(localPrefix + "/" + path)
      .catch(error => console.error('Error:', error));
  }

  
  var mediaQuery = window.matchMedia('(max-width: 600px)');
  if (mediaQuery.matches) {
    var sidebar = document.getElementById('sidebar');
    sidebar.addEventListener('click', function (event) {
      if (event.target.tagName === 'A') {
        toggleVisibility()
      }
    });

    document.getElementById("sidebar-rhs-button").style.display = "block"
  }

  
  
  
  const prefix = 'machine_learning\/machine_learing_theory\/'
  var base = document.getElementsByTagName("main")[0]

  const wrapLink = link => {
    var pre = prefix
    if (link.includes("http")) {
      return link
    }
    if (link.startsWith("#")) {
      return link
    }
    if (link.startsWith("/")) {
      return link
    }
    if (prefix !== '/') {
      pre = '/' + prefix
    }
    link = pre + link
    link = link.replace(".md", "")
    return link
  }

  Array.from(base.getElementsByTagName('img')).forEach(item => {
    item.setAttribute('src', wrapLink(item.getAttribute('src')))
  })
  Array.from(base.getElementsByTagName('a')).forEach(item => {
    let old = item.getAttribute('href')
    let link = wrapLink(old)
    if (old !== link && link.includes(".")) {
      localFunc.push(() => {
        item.addEventListener('click', event => {
          event.preventDefault()
          fetch(localPrefix + link)
            .catch(error => console.error('Error:', error));
        })
      })
    }
    item.setAttribute('href', link)
  })

  
  
  
  var main = document.getElementsByTagName("main")[0]
  document.querySelectorAll('a[href^="#"]').forEach((v, _1, _2) => {
    v.addEventListener('click', function (e) {
      e.preventDefault();
      var targetId = this.getAttribute("href").slice(1);
      var targetElement = document.getElementById(targetId);
      if (targetElement) {
        main.scrollTo({
          top: targetElement.offsetTop - 54,
          behavior: 'smooth'
        });
      }
    })
  });

  
  
  
  main.addEventListener('scroll', function () {
    var sections = document.querySelectorAll('main h2, main h3');
    var scrollPosition = main.scrollTop + 54;
    for (var i = 0; i < sections.length; i++) {
      var currentSection = sections[i];
      if (scrollPosition >= sections[i].offsetTop && (i + 1 >= sections.length || scrollPosition < sections[i + 1].offsetTop)) {
        document.querySelector('#sidebar #TableOfContents a[href="#' + currentSection.id + '"]').style.fontWeight = 'bold';
      } else {
        document.querySelector('#sidebar #TableOfContents a[href="#' + currentSection.id + '"]').style.fontWeight = 'normal';
      }

    }
  });


  
  
  
  window.addEventListener('scroll', function () {
    window.scrollTo({ top: 0 })
  });
  window.addEventListener('DOMContentLoaded', (event) => {
    new PagefindUI({ element: "#search", showSubResults: true });
  });

</script>

</html>