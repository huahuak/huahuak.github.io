@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@misc{wiki:xxx,
  author = {维基百科},
  title  = {幻觉 (人工智能) --- 维基百科{,} 自由的百科全书},
  year   = {2023},
  url    = {-{R|https://zh.wikipedia.org/w/index.php?title=%E5%B9%BB%E8%A7%89_(%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD)&oldid=79883124}-},
}
 
@inproceedings{nallapati2017summarunner,
  title     = {Summarunner: A recurrent neural network based sequence model for extractive summarization of documents},
  author    = {Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume    = {31},
  number    = {1},
  year      = {2017}
}

@article{rush2015neural,
  title   = {A neural attention model for abstractive sentence summarization},
  author  = {Rush, Alexander M and Chopra, Sumit and Weston, Jason},
  journal = {arXiv preprint arXiv:1509.00685},
  year    = {2015}
}

@inproceedings{varges2012semscribe,
  title     = {SemScribe: Natural Language Generation for Medical Reports.},
  author    = {Varges, Sebastian and Bieler, Heike and Stede, Manfred and Faulstich, Lukas C and Irsig, Kristin and Atalla, Malik},
  booktitle = {LREC},
  pages     = {2674--2681},
  year      = {2012}
}

@inproceedings{boag2020baselines,
  title        = {Baselines for chest x-ray report generation},
  author       = {Boag, William and Hsu, Tzu-Ming Harry and McDermott, Matthew and Berner, Gabriela and Alesentzer, Emily and Szolovits, Peter},
  booktitle    = {Machine learning for health workshop},
  pages        = {126--140},
  year         = {2020},
  organization = {PMLR}
}

@inproceedings{wang2022task,
  title     = {Task-oriented dialogue system as natural language generation},
  author    = {Wang, Weizhi and Zhang, Zhirui and Guo, Junliang and Dai, Yinpei and Chen, Boxing and Luo, Weihua},
  booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages     = {2698--2703},
  year      = {2022}
}

@inproceedings{wei2018task,
  title     = {Task-oriented dialogue system for automatic diagnosis},
  author    = {Wei, Zhongyu and Liu, Qianlong and Peng, Baolin and Tou, Huaixiao and Chen, Ting and Huang, Xuan-Jing and Wong, Kam-Fai and Dai, Xiang},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages     = {201--207},
  year      = {2018}
}

@article{ni2023recent,
  title     = {Recent advances in deep learning based dialogue systems: A systematic survey},
  author    = {Ni, Jinjie and Young, Tom and Pandelea, Vlad and Xue, Fuzhao and Cambria, Erik},
  journal   = {Artificial intelligence review},
  volume    = {56},
  number    = {4},
  pages     = {3055--3155},
  year      = {2023},
  publisher = {Springer}
}

@inproceedings{schwenk2012continuous,
  title     = {Continuous space translation models for phrase-based statistical machine translation},
  author    = {Schwenk, Holger},
  booktitle = {Proceedings of COLING 2012: Posters},
  pages     = {1071--1080},
  year      = {2012}
}

@article{Bengio,
  author     = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
  title      = {A Neural Probabilistic Language Model},
  year       = {2003},
  issue_date = {3/1/2003},
  publisher  = {JMLR.org},
  volume     = {3},
  number     = {null},
  issn       = {1532-4435},
  abstract   = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  journal    = {J. Mach. Learn. Res.},
  month      = {mar},
  pages      = {1137–1155},
  numpages   = {19}
}

@article{Mcroy,
  author  = {Mcroy, Susan and Channarukul, Songsak and Ali, Syed},
  year    = {2003},
  month   = {12},
  pages   = {381 - 420},
  title   = {An augmented template-based approach to text realization},
  volume  = {9},
  journal = {Natural Language Engineering},
  doi     = {10.1017/S1351324903003188}
}

@misc{gatt2018survey,
  title         = {Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation},
  author        = {Albert Gatt and Emiel Krahmer},
  year          = {2018},
  eprint        = {1703.09902},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@article{reiter1997building,
  title     = {Building applied natural language generation systems},
  author    = {Reiter, Ehud and Dale, Robert},
  journal   = {Natural Language Engineering},
  volume    = {3},
  number    = {1},
  pages     = {57--87},
  year      = {1997},
  publisher = {Cambridge university press}
}

@article{李雪晴2021自然语言生成综述,
  title   = {自然语言生成综述},
  author  = {李雪晴 and 王石 and 王朱君 and 朱俊武},
  journal = {计算机应用},
  volume  = {41},
  number  = {5},
  pages   = {1227},
  year    = {2021}
}

@misc{bengio2003neural,
  title     = {A neural probabilistic language model. journal of machine learning research, Vol. 3, No},
  author    = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  year      = {2003},
  publisher = {Feb}
}

@inproceedings{langkilde2000forest,
  title     = {Forest-based statistical sentence generation},
  author    = {Langkilde, Irene},
  booktitle = {1st Meeting of the North American Chapter of the Association for Computational Linguistics},
  year      = {2000}
}

@article{hopfield1982neural,
  title     = {Neural networks and physical systems with emergent collective computational abilities.},
  author    = {Hopfield, John J},
  journal   = {Proceedings of the national academy of sciences},
  volume    = {79},
  number    = {8},
  pages     = {2554--2558},
  year      = {1982},
  publisher = {National Acad Sciences}
}

@article{hochreiter1997long,
  title     = {Long short-term memory},
  author    = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal   = {Neural computation},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  year      = {1997},
  publisher = {MIT press}
}

@article{schmidhuber2015deep,
  title     = {Deep learning in neural networks: An overview},
  author    = {Schmidhuber, J{\"u}rgen},
  journal   = {Neural networks},
  volume    = {61},
  pages     = {85--117},
  year      = {2015},
  publisher = {Elsevier}
}

@article{cho2014learning,
  title   = {Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author  = {Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal = {arXiv preprint arXiv:1406.1078},
  year    = {2014}
}

@incollection{jordan1997serial,
  title     = {Serial order: A parallel distributed processing approach},
  author    = {Jordan, Michael I},
  booktitle = {Advances in psychology},
  volume    = {121},
  pages     = {471--495},
  year      = {1997},
  publisher = {Elsevier}
}

@inproceedings{mikolov2010recurrent,
  title        = {Recurrent neural network based language model.},
  author       = {Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle    = {Interspeech},
  volume       = {2},
  number       = {3},
  pages        = {1045--1048},
  year         = {2010},
  organization = {Makuhari}
}

@inproceedings{sutskever2011generating,
  title     = {Generating text with recurrent neural networks},
  author    = {Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
  booktitle = {Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages     = {1017--1024},
  year      = {2011}
}

@article{sutskever2014sequence,
  title   = {Sequence to sequence learning with neural networks},
  author  = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal = {Advances in neural information processing systems},
  volume  = {27},
  year    = {2014}
}

@article{bahdanau2014neural,
  title   = {Neural machine translation by jointly learning to align and translate},
  author  = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal = {arXiv preprint arXiv:1409.0473},
  year    = {2014}
}

@inproceedings{zhang2014chinese,
  title     = {Chinese poetry generation with recurrent neural networks},
  author    = {Zhang, Xingxing and Lapata, Mirella},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {670--680},
  year      = {2014}
}

@article{radford2018improving,
  title     = {Improving language understanding by generative pre-training},
  author    = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year      = {2018},
  publisher = {OpenAI}
}

@article{devlin2018bert,
  title   = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2018}
}

@article{ji2023survey,
  title     = {Survey of hallucination in natural language generation},
  author    = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal   = {ACM Computing Surveys},
  volume    = {55},
  number    = {12},
  pages     = {1--38},
  year      = {2023},
  publisher = {ACM New York, NY}
}

@article{elman1990finding,
  title     = {Finding structure in time},
  author    = {Elman, Jeffrey L},
  journal   = {Cognitive science},
  volume    = {14},
  number    = {2},
  pages     = {179--211},
  year      = {1990},
  publisher = {Wiley Online Library}
}

@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@article{stahlberg2020neural,
  title   = {Neural machine translation: A review},
  author  = {Stahlberg, Felix},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {69},
  pages   = {343--418},
  year    = {2020}
}