<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>/big_data/spark/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/big_data/spark/</guid>
      <description>Spark RDD Programming A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel.&#xA;简单而言，RDD 可以视作分布式环境下的数组，拥有高效、稳定的优点。RDD 通过内存计算、分区策略 (Partition) 与调度优化器 (Scheduler) 以实现高效的并行计算，RDD 不可变性特点便于实现部分缓存备份，容错性使其在分布式环境下具备稳定的优点。&#xA;Example https://github.com/apache/spark/blob/master/examples/src/main/python/wordcount.py&#xA;# word count lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0]) counts = lines.flatMap(lambda x: x.split(&amp;#39; &amp;#39;)) \ .map(lambda x: (x, 1)) \ .reduceByKey(add) output = counts.collect() for (word, count) in output: print(&amp;#34;%s: %i&amp;#34; % (word, count)) Representation In a nutshell, we propose representing each RDD through a common interface that exposes five pieces of information: a set of partitions, which are atomic pieces of the dataset; a set of dependencies on parent RDDs; a function for computing the dataset based on its parents; and metadata about its partitioning scheme and data placement.</description>
    </item>
    <item>
      <title></title>
      <link>/daily-note/2022-09-15/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/daily-note/2022-09-15/</guid>
      <description>Daily-Note 09-15 OVERVIEW 在分布式系统中，分布式高并发编程模型、socket 网络模型、IO 模型等属于了分布式系统的基石。&#xA;Actor Model，一种高性能分布式并发编程模型，来自 Erlang 语言。背景介绍 Netty, Netty is a NIO client server framework which enables quick and easy development of network applications such as protocol servers and clients. It greatly simplifies and streamlines network programming such as TCP and UDP socket server. select / poll、信号驱动 IO、epoll，多种非阻塞的操作系统 IO 模型。 Java NIO，与 java 1.4 之前的 BIO 相比，NIO 提供了非阻塞的 IO 模型，大大提高了 Java 编程语言在大型应用服务器上的 IO 性能。 Actor Model 我们为什么需要 Actor 编程模型？</description>
    </item>
    <item>
      <title></title>
      <link>/daily-note/2022-09-18/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/daily-note/2022-09-18/</guid>
      <description>Daily-Note 09-18 OVERVIEW Gin，go web 框架。 Go MemModel，go 语言内存模型。 Go Sync，Go 并发包，提供 Atomic、cond、once、pool、mutex、rwmutex、waitgroup、map 等线程并发工具。 Go container，提供了 List、Ring、Heap 等容器，此外 go builtin 包括 slice、map 等多种数据结构。 IPC sig，进程通信中 sig 是通过信号的发送，回调处理函数来实现进程间的通信。 Gin Route gin框架路由详解&#xA;LeetCode 208.实现 Trie(前缀树)&#xA;// -- main.go -- // engine := gin.Default() engine.GET(&amp;#34;/test/abc&amp;#34;) // -- routergroup.go -- // // GET is a shortcut for router.Handle(&amp;#34;GET&amp;#34;, path, handle). func (group *RouterGroup) GET(relativePath string, handlers ...HandlerFunc) IRoutes { return group.handle(http.MethodGet, relativePath, handlers) } func (group *RouterGroup) handle(httpMethod, relativePath string, handlers HandlersChain) IRoutes { absolutePath := group.</description>
    </item>
    <item>
      <title></title>
      <link>/daily-note/2022-10-04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/daily-note/2022-10-04/</guid>
      <description>Daily-Note 10-04 OVERVIEW Spark 并发模型，Spark 以 Executor 进程为单元，在 task 上实现多线程并发模型。 线性模型，传统机器学习模型。 字典树，一种前缀树，可用于路由匹配、语法补全等场景。 拉格朗日乘子法，数学最优化问题中求解多元函数在约束条件下局部极值的方法。 Spark 并发模型 RDD RDD 是 Resilient Distrubuted Dataset，弹性分布式数据集，是 Spark 数据的基本组织方式。&#xA;// -- RDD.scala -- // final def iterator(split: Partition, context: TaskContext): Iterator[T] = { if (storageLevel != StorageLevel.NONE) { getOrCompute(split, context) } else { computeOrReadCheckpoint(split, context) } } private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = { if (isCheckpointedAndMaterialized) { firstParent[T].iterator(split, context) } else { compute(split, context) } } def compute(split: Partition, context: TaskContext): Iterator[T] 调用 iterator() 方法将触发 RDD 的计算方法，getOrCompute 和 computeOrReadCheckpoint 方法实现了计算结果缓存的机制，其最终的计算方法落在 compute 方法，该方法由 RDD 的子类实现具体的计算方法。</description>
    </item>
    <item>
      <title></title>
      <link>/daily-note/2022-10-24/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/daily-note/2022-10-24/</guid>
      <description>Daily-Note 10-24 OVERVIEW Pytorch，机器学习框架 神经网络，由神经元构成的非线性参数模型。 SparkPlanner，将 Logical Plan 转换到 Physics Plan。 Spark Expression， Pytorch TUTORIAL 1: INTRODUCTION TO PYTORCH fsdl-text-recognizer-2021-labs&#xA;神经网络 (Neural Network)&#xA;Tutorial 1: Introduction to PyTorch&#xA;BP (Back Propagation)&#xA;SparkPlanner Spark SQL Query Engine Deep Dive (7) – Spark Planner Overview&#xA;QueryPlan，QueryPlan 继承自 TreeNode，其增加了逻辑图和物理图的一些公共属性和遍历方法。&#xA;LogicalPlan，LogicalPlan 是逻辑图中如 Aggregate/Sort/Join 等节点的基类。其基础自 QueryPlan。&#xA;SparkPlan，SparkPlan 是物理算子的基类，物理算子包括 HashAggregateExec、ShuffleExchangeExec 等。与 LogicalPlan 相同，继承自 QueryPlan。&#xA;QueryExecution 包括一个请求中主要流程的各个阶段，包括 UnresovledPlan Logical Plan/Logical Plan/Optimized Logical Plan/Physical Plan 等阶段。在 QueryExecution 中分别对应为 logical/analyzed/optimized/sparkPlan 等属性。</description>
    </item>
    <item>
      <title></title>
      <link>/daily-note/2022-11-02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/daily-note/2022-11-02/</guid>
      <description>Daily-Note 11-02 OVERVIEW Kafka，分布式高性能事件流式处理平台。 Docker，docker 是一个用于部署、传输、运行应用的开发平台。 Spark Shuffle，分布式环境下重新组织数据使有序的操作。 Kafka Kafka introduction&#xA;Kafka documentation&#xA;Docker docker&#xA;Docker 从入门到实践&#xA;EntryPoint，该命令相比于 CMD 命令而言，可以在 docker run 命令后添加参数。EntryPoint&#xA;Spark Shuffle Spark SQL Query Engine Deep Dive (14) – Partitioning &amp;amp; Ordering&#xA;Spark SQL Query Engine Deep Dive (15) – UnsafeExternalSorter &amp;amp; SortExec&#xA;Spark SQL Query Engine Deep Dive (16) – ShuffleExchangeExec &amp;amp; UnsafeShuffleWriter&#xA;Shuffle 目的在于使 Map 端的输出重新组织，从而使得数据根据 Key 有序，方便后续的 Reduce 端进行数据聚合。在特定情况下，聚合也会提前在 Map 端执行，从而减少数据传输的大小。&#xA;Overview，在物理计划 SparkPlan 由 Optimized 转换完成之后，进入 preparations 阶段，生成最后的 executedPlan，在 preparations 中的 EnsureRequirements 规则被应用，从而在需要 Shuffle 的算子之间插入对应的 SparkPlan，主要是 ShuffleExchangeExec。</description>
    </item>
    <item>
      <title></title>
      <link>/daily-note/2022-11-20/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/daily-note/2022-11-20/</guid>
      <description>Daily-Note 11-20 OVERVIEW Apache Arrow，用于高性能大数据处理传输场景。 Apache Arrow https://arrow.apache.org</description>
    </item>
    <item>
      <title></title>
      <link>/draft/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/draft/</guid>
      <description>Draft Linux disk manage Linux 扩容 / 根分区(LVM+非LVM) from 知乎&#xA;Ubuntu update-alternatives blog4hua markdown to html, with code highlight and latex render.&#xA;markdown internal link convert to html link.&#xA;packet all file into single html.</description>
    </item>
    <item>
      <title></title>
      <link>/home/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/home/</guid>
      <description>Home I am a graduate student and my interests include big data, neural networks, and web development.</description>
    </item>
    <item>
      <title></title>
      <link>/machine_learning/deep_learning_theory/deep-learning-theory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/machine_learning/deep_learning_theory/deep-learning-theory/</guid>
      <description>Deep Learning Theory Paper Reading Training with Noise is Equivalent to Tikhonov Regularization Neural Computation (1995) #Dropout&#xA;1995年，克里斯托弗·毕晓普证明了具有输入噪声的训练等价于Tikhonov正则化 (Bishop, 1995)。 这项工作用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系。&#xA;Dropout: A Simple Way to Prevent Neural Networks from Overfitting JMLR 2014 #Dropout&#xA;斯里瓦斯塔瓦等人 (Srivastava et al., 2014) 就如何将毕晓普的想法应用于网络的内部层提出了一个想法：在训练过程中，他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。&#xA;Understanding the difficulty of training deep feedforward neural networks AISTATS 2010 #Parameter Init&#xA;这就是现在标准且实用的Xavier初始化的基础， 它以其提出者 (Glorot and Bengio, 2010) 第一作者的名字命名。 通常，Xavier初始化从均值为零，方差的高斯分布中采样权重。&#xA;Batch Normalization Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift ICML 2015 Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change.</description>
    </item>
    <item>
      <title></title>
      <link>/machine_learning/machine_learing_theory/machine_learning_theory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/machine_learning/machine_learing_theory/machine_learning_theory/</guid>
      <description>Machine Learning Theory Reference CS229 Syllabus&#xA;cs229_lecture_notes.pdf&#xA;Linear Regression §1 prior knowledge Maximum Likelihood Estimation (Maximum Likelihood Estimation，MLE)&#xA;似然函数 Wiki&#xA;Likelihood function Wiki&#xA;极大似然估计 Wiki&#xA;Matrix derivatives The exponential family Exponential family Wiki&#xA;Multinomial Distribution 多项分布的概率分布函数为：&#xA;$$ \mathrm{P}\left(\mathrm{X}{1}=\mathrm{k}{1},\mathrm{X}{2}=\mathrm{k}{2},\cdots,\mathrm{X}{\mathrm{n}}=\mathrm{k}{\mathrm{n}}\right)=\dfrac{\mathrm{n}!}{\mathrm{k}{1}!\mathrm{k}{2}!\cdots\mathrm{k}{\mathrm{n}}!}\prod{i=1}^{\mathrm{n}}\mathrm{p}{i}^{\mathrm{k}{i}}\quad,\sum_{i=1}^{\mathrm{n}}\mathrm{k}_{i}=\mathrm{n} $$ §2 Linear Regression Object function:&#xA;$$ h(x) = \theta^{\mathrm{T}}x; $$ Cost function:&#xA;$$ J(\theta) = \frac{1}{2} \sum\limits_{i=1}^{n} (h_\theta(x_i) - y_i)^2 $$ 方法一，通过 gradient descent 方法调整参数 $\theta$。&#xA;首先计算梯度方向，&#xA;$$ \begin{aligned} \frac{\partial}{\partial\theta_j}J(\theta) &amp;=\frac{\partial}{\partial\theta_j}\frac{1}{2}\left(h_\theta(x)-y\right)^2\\ &amp;=2\cdot\frac{1}{2}\left(h_\theta(x)-y\right)\cdot\frac{\partial}{\partial\theta_j}\left(h_\theta(x)-y\right)\\ &amp;=\left(h_\theta(x)-y\right)\cdot\frac{\partial}{\partial\theta_j}\left(\sum_{i=0}^{d}\theta_ix_i-y\right)\\ &amp;=\left(h_\theta(x)-y\right)x_j \end{aligned} $$ 根据下式迭代调整 $\theta$ 参数。</description>
    </item>
    <item>
      <title></title>
      <link>/machine_learning/math/probability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/machine_learning/math/probability/</guid>
      <description>Probability cs229-probability_review.pdf&#xA;§1 Probability Introduction Element of Probability 样本空间 (Sample Space)&#xA;是指随机现象所有的基本结果组成的集合。&#xA;事件空间 (Event Space)&#xA;是指样本空间所有子集合所构成的集合&#xA;Permutation &amp;amp; Combination Combinatorics -Wiki&#xA;排列 (permutation) 指从 n 个元素按次序选取 r 个元素组成一组，定义为&#xA;$$ P_n^r=n\times(n-1)\times\cdots\times(n-r+1)=\frac{n!}{(n-r!)}. $$ 组合 (combination) 指从 n 个不同的元素中任意抽取 r 个元素组成一组，定义为&#xA;$$ C_n^r = \binom nr=\frac{P_n^r}{r!}=\frac{n(n-1)\cdots(n-r+1)}{r!}=\frac{n!}{r!(n-r)!}. $$ Conditional Probability $$ P(A|B)\triangleq\frac{P(A\cap B)}{P(B)} $$ 条件概率即已知事件 B 发生的情况下，事件 A 发生的概率。&#xA;Law of total probability 假设 $S = \{1, 2, ..., k\}$ ，若有&#xA;$$ A_i, i \in S, P(\bigcap_{i \in S&#39;}A_i) = \varnothing \\ B \subseteq \bigcup_{i \in S} A_i $$ 则有</description>
    </item>
    <item>
      <title></title>
      <link>/machine_learning/natural_language_generation/natural_language_generation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/machine_learning/natural_language_generation/natural_language_generation/</guid>
      <description>Natural Language Generation Recurrent Neural Network Milestone papers Paper Comments Source serial_order_a_parallel_distrmuted_processing_approach.pdf Jordan RNN 1986 finding_structure_in_time.pdf Elman RNN 1990 long_short_term_memory.pdf LSTM 1997 NIPS-2014-sequence-to-sequence-learning-with-neural-networks-Paper.pdf Seq2Seq NIPS-2014 learning_phrase_representations_using_rnn_encoder&#xA;_decoder_for_statistical_machine_translation.pdf Encoder-Decoder neural_machine_translation_by_jointly_learning&#xA;_to_align_and_translate.pdf Attention attention_is_all_you_need.pdf Transformer improving_language_understanding_&#xA;by_generative_pre_training.pdf Generative Pre Traning Paper Reading Paper Comments A Survey of Natural Language Generation Techniques with a Focus on Dialogue Systems - Past, Present and Future Directions.pdf 自然语言生成综述.pdf Building applied natural language generation systems.pdf Forest-Based Statistical Sentence Generation.</description>
    </item>
    <item>
      <title></title>
      <link>/os_note/os_note/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/os_note/os_note/</guid>
      <description>Operate System Note Overview Cache Paper Comments MESIF-2009.pdf cache coherence protocol </description>
    </item>
    <item>
      <title></title>
      <link>/programming_languages/algorithm/algorithm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/programming_languages/algorithm/algorithm/</guid>
      <description>Algorithm Two Sum Two Sum HashMap Given an array of integers nums&amp;nbsp;and an integer target, return indices of the two numbers such that they add up to target.&#xA;You may assume that each input would have exactly one solution, and you may not use the same element twice.&#xA;You can return the answer in any order.&#xA;Example 1:&#xA;Input: nums = [2,7,11,15], target = 9 Output: [0,1] Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].</description>
    </item>
    <item>
      <title></title>
      <link>/programming_languages/c&#43;&#43;/c&#43;&#43;/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/programming_languages/c&#43;&#43;/c&#43;&#43;/</guid>
      <description>C++ Note Overload </description>
    </item>
    <item>
      <title></title>
      <link>/programming_languages/golang/golang/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/programming_languages/golang/golang/</guid>
      <description>Go quick start Ref Go 语言设计与实现&#xA;TODO select/defer/panic&amp;amp;recover Timer: Go 定时器实现 内存分配器/垃圾收集器/栈内存管理 Json/HTTP/数据库等标准库 同步原语拓展部分 ErrGroup/Semaphore/SingleFlight 深入理解 GMP 模型、网络轮询器 Go Lambda 变量逃逸 nil and &amp;hellip;args? sync.pool 原理 语法基础 for &amp;amp; range for range 是 Go 中常用的范围遍历方法，在 Go 的实现中，会将 for range 转换为普通的 for 循环进行处理。&#xA;数组和切片，数组和切片可以通过 for range 进行遍历，可细分为三种不同的遍历，即是否使用 range 返回的 index 和 value。&#xA;arr := [...]int{1, 2, 3} for range arr {} for _ = range arr {} for _, _ = range arr {} 在 for range 实现中，会先通过 len( ) 方法获取 arr 的长度，作为 for 的遍历次数，因此在 for range 中对 arr 进行修改不能够改变 for range 的遍历次数。如下示例所示。</description>
    </item>
    <item>
      <title></title>
      <link>/project/ohmymac/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/project/ohmymac/</guid>
      <description>ohmymac ohmymac is a menubar-only application, which is used to extend functionality of mac.&#xA;ohmymac is currently under development.&#xA;Features Window Design Philosophy When using MacBook, I will lose direction in multiple windows and spaces, Because window and Space switch logic is so complex. For Example, when I change to another space, I will forget the old windows when need to go back.&#xA;So I need to remember the most recent windows (just record the order of the openning windows) globally.</description>
    </item>
    <item>
      <title></title>
      <link>/project/soteria/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/project/soteria/</guid>
      <description>Soteria Soteria1 is a secure neural network framework based on trusted hardware.&#xA;Overview Soteria Wiki&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
    </item>
    <item>
      <title></title>
      <link>/project/what_do_you_need_in_a_new_computer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/project/what_do_you_need_in_a_new_computer/</guid>
      <description>What do you need in a new computer? If you have a new MacBook/Linux Computer&amp;hellip;&#xA;ohmyzsh You need install ohmyzsh on you new MacBook.&#xA;https://ohmyz.sh&#xA;sh -c &amp;#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&amp;#34; and you need install some plugin.&#xA;https://github.com/zsh-users/zsh-autosuggestions/blob/master/INSTALL.md&#xA;git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions # add below config to .zshrc plugins=( # other plugins... zsh-autosuggestions ) https://github.com/zsh-users/zsh-syntax-highlighting&#xA;git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting # add beblow config to .zshrc plugins=( [plugins...] zsh-syntax-highlighting) Data Science Env Pytorch https://pytorch.</description>
    </item>
    <item>
      <title></title>
      <link>/research/llm/large_language_model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/research/llm/large_language_model/</guid>
      <description>Large Language Model PEFT Adapter Layers nips_2017_learning_multiple_visual_domains_with_residual_adapters.pdf&#xA;pmlr_2019_parameter_efficient_transfer_learning_for_nlp.pdf&#xA;prefix tunning prefix_tuning_optimizing_continuous_prompts_for_generation.pdf&#xA;LoRA LORA: LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODELS arXiv 2021 Abstract: We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.&#xA;We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.</description>
    </item>
    <item>
      <title></title>
      <link>/research/ppml/privacy_perserving_machine_learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/research/ppml/privacy_perserving_machine_learning/</guid>
      <description>Privacy-Perserving Machine Learning Paper A FAST, PERFORMANT, SECURE DISTRIBUTED TRAINING FRAMEWORK FOR LLM ICASSP 2024 Abstract: The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data. How- ever, maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved. In this paper, we propose a secure distributed LLM based on model slicing. In this case, we deploy the Trusted Execution Environment (TEE) on both the client and server side, and put the fine-tuned structure (LoRA or embedding of P-tuning v2) into the TEE.</description>
    </item>
    <item>
      <title></title>
      <link>/research/ppml/task_for_ppml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/research/ppml/task_for_ppml/</guid>
      <description>Task For Privacy-Preserving Machine Learning Confidential Computing Privacy-Preserving Machine Learning 目标： 学习隐私保护的机器学习相关论文方法 (重点)。&#xA;Paper Comment Source Todo shadownet_a_secure_and_efficient_on_&#xA;device_model_inference_system_for_&#xA;convolutional_neural_networks.pdf SP 2023 goten_gpu_outsourcing_trusted_execution&#xA;_of_neural_network_training.pdf AAAI 2021 oblivious_multi_party_machine_learning_on&#xA;_trusted_processors.pdf 2016 SS slalom_fast_verifiable_and_private_execution&#xA;_of_neural_networks_in_trusted_hardware.pdf Slalom, recently proposed by Tramer and Boneh, is the first solution that leverages both GPU (for efficient batch computation) and a trusted execution environment (TEE) (for minimizing the use of cryptography). ICLR 2019 ✅ delphi_a_cryptographic_inference_service_&#xA;for_neural_networks.pdf Delphi is based on Gazelle and uses homomorphic encryption, grabled circuits, and secret shares to achieve client and server privacy protection in neural network.</description>
    </item>
  </channel>
</rss>
